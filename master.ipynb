{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "from pathlib import Path\n",
    "\n",
    "def extract_files(start, extract_extention, end):\n",
    "    # z = Path(f'data/load')\n",
    "    z = Path(f'{start}')\n",
    "    b = list(z.glob('*.zip'))\n",
    "    for i in b:\n",
    "        with ZipFile(i, 'r') as zip:\n",
    "            listOfFileNames = zip.namelist()\n",
    "            for fileName in listOfFileNames:\n",
    "                if fileName.endswith(f'.{extract_extention}'):\n",
    "                    zip.extractall(f'{end}')\n",
    "\n",
    "extract_files('data/load', 'csv', 'temp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "### add to log module ###\n",
    "logging.basicConfig(filename='app.log',\n",
    "                    filemode='w',\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                    level=logging.INFO,\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "def df_len(name, df):\n",
    "    logging.info(f'len: {len(df)} \\t\\t {name}')\n",
    "\n",
    "### import into main() ###\n",
    "# import log.log as log\n",
    "# log.df_len()\n",
    "\n",
    "load = pd.DataFrame()\n",
    "df_len('load', load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, timedelta, datetime\n",
    "import holidays\n",
    "\n",
    "today = date.today()\n",
    "HOLIDAYS_US = holidays.US(years= today.year)\n",
    "HOLIDAYS_company = dict(zip(HOLIDAYS_US.values(), HOLIDAYS_US.keys()))\n",
    "\n",
    "del_list = (\"Washington\\'s Birthday\", 'Juneteenth National Independence Day','Columbus Day','Veterans Day')\n",
    "for i in del_list:\n",
    "    HOLIDAYS_company.pop(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ONE_DAY = timedelta(days=1)\n",
    "\n",
    "def next_business_day(start):\n",
    "    next_day = start + ONE_DAY\n",
    "    while next_day.weekday() in holidays.WEEKEND or next_day in HOLIDAYS_company:\n",
    "        next_day += ONE_DAY\n",
    "    return next_day\n",
    "\n",
    "def last_business_day(start):\n",
    "    next_day = start - ONE_DAY\n",
    "    while next_day.weekday() in holidays.WEEKEND or next_day in HOLIDAYS_company:\n",
    "        next_day -= ONE_DAY\n",
    "    return next_day\n",
    "\n",
    "def x_Bus_Day_ago(N):\n",
    "    B10 = []\n",
    "    seen = set(B10)\n",
    "    i = today\n",
    "\n",
    "    while len(B10) < N:\n",
    "        item = last_business_day(i)\n",
    "        if item not in seen:\n",
    "            seen.add(item)\n",
    "            B10.append(item)\n",
    "        i -= timedelta(days=1)\n",
    "    return B10[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I've found pandas.isin or set.intersection()\n",
    "# Are very comparable\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "def list_in_column(df, list):\n",
    "    filter0 = df['col_name'].isin(list)\n",
    "    df['new_col'] = np.where(filter0, 1, 0)\n",
    "    return df\n",
    "\n",
    "# pandas_df     = list_add['OutreachID'].squeeze()\n",
    "# pandas_series = list_add['OutreachID'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_column(df):\n",
    "    df['OutreachID'] = df['OutreachID'].astype(str)\n",
    "    df['Matches'] = df.groupby(['PhoneNumber'])['OutreachID'].transform(lambda x : '|'.join(x)).apply(lambda x: x[:3000])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.csv as csv\n",
    "\n",
    "def data_proccess(location):\n",
    "    z = Path(f'{location}')\n",
    "    b = list(z.glob(f'*.csv'))\n",
    "\n",
    "    final = pd.DataFrame()\n",
    "    for i in b:\n",
    "        table = csv.read_csv(i)\n",
    "        df = table.to_pandas()\n",
    "        ### get name of file\n",
    "        # st = (str(i).split('\\\\')[-1][:-4])\n",
    "        st = 'file_name'\n",
    "        \n",
    "        ### filter what you want\n",
    "        # today = datetime.strptime(st, \"%Y-%m-%d\")\n",
    "        # yesterday = last_business_day(today)\n",
    "        filter1 = 'yesterday'\n",
    "        filter2 = 'properly'\n",
    "\n",
    "        ### create list\n",
    "        pastdue = df[df.Outreach_Status == 'Past Due'][['OutreachID']]\n",
    "\n",
    "        worked = df[df.Last_Call == filter1]\n",
    "\n",
    "        worked_ls = worked['OutreachID'].tolist()\n",
    "\n",
    "        worked_properly = worked[worked.Outreach_Status == filter2]\n",
    "        worked_properly_ls = worked_properly['OutreachID'].tolist()\n",
    "        try:\n",
    "            ### try and skip first file, second file uses \"last\"\n",
    "            print(st)\n",
    "            total_work   = last[last.OutreachID.isin(worked_ls)]\n",
    "            total_proper = last[last.OutreachID.isin(worked_properly_ls)]\n",
    "\n",
    "            total        = len(last)\n",
    "            work         = len(total_work)\n",
    "            count        = len(total_proper)\n",
    "            pct = count / work\n",
    "            \n",
    "            final[f'{st}'] = [total, work, count, pct]\n",
    "            last = pastdue\n",
    "        except:\n",
    "            last = pastdue\n",
    "    return final\n",
    "final = data_proccess('temp')\n",
    "final = final.T\n",
    "final.columns = ['Total PastDue', 'Total Worked', 'Next Day Schedule', '%']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pivot_tables(df):\n",
    "    df.pivot_table(index =['Daily_Priority', 'Daily_Groups', 'rolled'], \n",
    "                    columns ='Skill', \n",
    "                    values ='PhoneNumber', \n",
    "                    aggfunc = ['count'], \n",
    "                    margins=True,\n",
    "                    margins_name= 'TOTAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and -> &\n",
    "# or  -> | \n",
    "# not -> !=\n",
    "# < , > , <= , >=\n",
    "\n",
    "def np_filters(df, ifTrue):\n",
    "    # filters\n",
    "    f1 = df['col1'] == 'x'\n",
    "    f2 = df['col2'] == 'y'\n",
    "    # replace based on filer \n",
    "    # filter, if True, if False\n",
    "    df['Skill'] = np.where(f1 | f2, ifTrue, df['Skill'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Input/output static tables ###\n",
    "# supports module setup where you only need to add paths once\n",
    "# for example ./src/pipeline/tables \n",
    "# holds all the sources code, while \n",
    "# ./data/table_drops holds the static table\n",
    "# when I import this as a function into main everything is taken care of\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "paths = Path(__file__).parent.absolute().parent.absolute().parent.absolute()\n",
    "table_path   = paths / \"data/table_drop\"\n",
    "\n",
    "def tables(push_pull, table, name, path=table_path):\n",
    "    if push_pull == 'pull':\n",
    "        # return csv.read_csv(paths / path / name)\n",
    "        return pd.read_csv(path / name, sep=',', on_bad_lines='warn', engine=\"python\",)\n",
    "    else:\n",
    "        table.to_csv(table_path / name, sep=',', index=False)\n",
    "\n",
    "start = tables('pull', 'NA', 'start.csv')\n",
    "tables('push', start, 'start.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyodbc\n",
    "\n",
    "# ./src/server/queries/master_reporting.py\n",
    "def sql(name):\n",
    "\tsql = (f\"\"\"\n",
    "        SELECT *\n",
    "        FROM db.Prod.Master_Reporting AS mr\n",
    "        WHERE mr.names = '{name}'\n",
    "\t\"\"\")\n",
    "\treturn sql\n",
    "\n",
    "# ./src/server/query.py\n",
    "def query(servername, database, sql, query_name):\n",
    "      # create the connection\n",
    "      try:\n",
    "            conn = pyodbc.connect(f\"\"\"\n",
    "                  DRIVER={{SQL Server}};\n",
    "                  SERVER={servername};\n",
    "                  DATABASE={database};\n",
    "                  Trusted_Connection=yes\"\"\",\n",
    "                  autocommit=True) \n",
    "      except pyodbc.OperationalError:\n",
    "            print(\"\"\"Couldn\\'t connect to server\"\"\")\n",
    "            query(servername, database, sql, query_name)\n",
    "      else:\n",
    "            print(f'''Connected to Server \\t {query_name}''')\n",
    "            df = pd.read_sql(sql, conn)\n",
    "            return df\n",
    "\n",
    "# ./src/main.py\n",
    "MR_sql = server.queries.master_reporting.sql('aaron')\n",
    "master_reporting   = server.query.query(servername, \n",
    "                                        database,\n",
    "                                        MR_sql,\n",
    "                                        'master_reporting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Using aggrigation, collect top n and convert the remainder into one group.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "top = 10\n",
    "ascend = False    # it can also be smallest items, set ascend = True \n",
    "\n",
    "group   = 'col_name'\n",
    "agg_col = 'col_name'\n",
    "agg     = 'agg_type' # count, mean, sum, median, min, max, mode, std, var\n",
    "\n",
    "top_groups = df.groupby(group).agg({agg_col:agg}).sort_values(by=agg_col,ascending=ascend)\n",
    "top_groups = top_groups[:top].copy()\n",
    "top_groups.loc[f'Not Top {top}'] = top_groups[top:].sum()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e25b5007a15cbf93340b4f9138f1187145c0a10fd312404513e24be36619193e"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit ('offsite': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
